
loading annotations into memory...
23-07-10 19:58:57.691 - INFO:   name: sanitycheck
  phase: train
  gpu_ids: [1]
  path:[
    log: experiments/sanitycheck_230710_195854/logs
    checkpoint: experiments/sanitycheck_230710_195854/checkpoint
    resume_state: None
    json_dt: /home/zhuhe/HPE-with-Diffusion/config/dt_val_results.json
    json_gt: /home/zhuhe/HPE-with-Diffusion/config/gt_val_results.json
    experiments_root: experiments/sanitycheck_230710_195854
  ]
  datasets:[
    train:[
      type: mscoco
      root: /home/zhuhe/HPE-with-Diffusion/data/coco/
      img_prefix: images/train2017
      ann: annotations/person_keypoints_train2017.json
      aug:[
        flip: True
        rot_factor: 45
        scale_factor: 0.25
        num_joints_half_body: 3
        prob_half_body: 0.3
      ]
    ]
    val:[
      type: mscoco
      root: /home/zhuhe/HPE-with-Diffusion/data/coco/
      img_prefix: images/val2017
      ann: annotations/person_keypoints_val2017.json
      data_len: 3
    ]
    test:[
      type: mscoco_det
      root: /home/zhuhe/HPE-with-Diffusion/data/coco/
      img_prefix: images/val2017
      det_file: /home/zhuhe/HPE-with-Diffusion/config/test_det_rcnn.json
      ann: annotations/person_keypoints_val2017.json
    ]
  ]
  data_preset:[
    type: simple
    sigma: 2
    num_joints: 17
    image_size: [256, 192]
    heatmap_size: [64, 48]
  ]
  model:[
    regressor:[
      num_layers: 50
    ]
    denoise_transformer:[
      dim: 2048
      num_time_embeds: 1
      num_image_embeds: 4
      num_pose_embeds: 1
      num_keypoints: 17
      casual_transformer:[
        depth: 12
        dim_head: 64
        heads: 12
        ff_mult: 4
        norm_out: True
        attn_dropout: 0.05
        ff_dropout: 0.05
        final_proj: True
        normformer: True
        rotary_emb: True
      ]
    ]
    diffusion:[
      condition_on_preds: True
    ]
    beta_schedule:[
      train:[
        schedule: linear
        n_timestep: 2000
        linear_start: 1e-06
        linear_end: 0.01
      ]
      val:[
        schedule: linear
        n_timestep: 2000
        linear_start: 1e-06
        linear_end: 0.01
      ]
    ]
  ]
  loss:[
    type: sanity_check
    regress: l1
  ]
  train:[
    batch_size: 32
    end_epoch: 270
    optimizer: adam
    lr: 0.001
    lr_factor: 0.1
    lr_step: [170, 200]
    val_freq: 1000
    save_checkpoint_freq: 5000
    print_freq: 50
    ema_scheduler:[
      step_start_ema: 5000
      update_ema_every: 1
      ema_decay: 0.9999
    ]
  ]
  test:[
    heatmap2coord: coord
    batch_size: 32
  ]
  wandb:[
    project_name: DiffHPE
    run_name: sanitycheck
  ]
  distributed: False
Done (t=3.11s)
creating index...
index created!
23-07-10 19:59:05.700 - INFO: Initial Dataset Finished
loading annotations into memory...
Done (t=0.08s)
creating index...
index created!
/home/zhuhe/.conda/envs/diffhpe/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/zhuhe/.conda/envs/diffhpe/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
23-07-10 19:59:09.844 - INFO: Network G structure: GaussianDiffusion, with parameters: 719,051,762
23-07-10 19:59:09.844 - INFO: GaussianDiffusion(
  (regressor): Regressor(
    (preact): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (avg_pool): AdaptiveAvgPool2d(output_size=1)
    (fc_layer): Linear(
      (linear): Linear(in_features=2048, out_features=34, bias=True)
    )
  )
  (denoise_fn): DenoiseTransformer(
    (to_time_embeds): Sequential(
      (0): PositionalEncoding()
      (1): Linear(in_features=2048, out_features=4096, bias=True)
      (2): Swish()
      (3): Linear(in_features=4096, out_features=2048, bias=True)
      (4): Rearrange('b c (h w) -> b c h w', h=1)
      (5): Rearrange('b c h w -> b (c h) w')
    )
    (to_image_embeds): Sequential(
      (0): Linear(in_features=2048, out_features=8192, bias=True)
      (1): Rearrange('b (n d) -> b n d', n=4)
    )
    (to_pose_embeds): Sequential(
      (0): PositionalEncoding()
      (1): Linear(in_features=2048, out_features=4096, bias=True)
      (2): Swish()
      (3): Linear(in_features=4096, out_features=2048, bias=True)
      (4): Rearrange('b c (h w) -> b c h w', h=1)
      (5): Rearrange('b c h w -> b (c h) w')
    )
    (causal_transformer): CausalTransformer(
      (init_norm): Identity()
      (rel_pos_bias): RelPosBias(
        (relative_attention_bias): Embedding(32, 12)
      )
      (layers): ModuleList(
        (0): ModuleList(
          (0): Attention(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.05, inplace=False)
            (to_q): Linear(in_features=2048, out_features=768, bias=False)
            (to_kv): Linear(in_features=2048, out_features=128, bias=False)
            (rotary_emb): RotaryEmbedding()
            (to_out): Sequential(
              (0): Linear(in_features=768, out_features=2048, bias=False)
              (1): LayerNorm()
            )
          )
          (1): Sequential(
            (0): LayerNorm()
            (1): Linear(in_features=2048, out_features=16384, bias=False)
            (2): SwiGLU()
            (3): LayerNorm()
            (4): Dropout(p=0.05, inplace=False)
            (5): Linear(in_features=8192, out_features=2048, bias=False)
          )
        )
        (1): ModuleList(
          (0): Attention(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.05, inplace=False)
            (to_q): Linear(in_features=2048, out_features=768, bias=False)
            (to_kv): Linear(in_features=2048, out_features=128, bias=False)
            (rotary_emb): RotaryEmbedding()
            (to_out): Sequential(
              (0): Linear(in_features=768, out_features=2048, bias=False)
              (1): LayerNorm()
            )
          )
          (1): Sequential(
            (0): LayerNorm()
            (1): Linear(in_features=2048, out_features=16384, bias=False)
            (2): SwiGLU()
            (3): LayerNorm()
            (4): Dropout(p=0.05, inplace=False)
            (5): Linear(in_features=8192, out_features=2048, bias=False)
          )
        )
        (2): ModuleList(
          (0): Attention(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.05, inplace=False)
            (to_q): Linear(in_features=2048, out_features=768, bias=False)
            (to_kv): Linear(in_features=2048, out_features=128, bias=False)
            (rotary_emb): RotaryEmbedding()
            (to_out): Sequential(
              (0): Linear(in_features=768, out_features=2048, bias=False)
              (1): LayerNorm()
            )
          )
          (1): Sequential(
            (0): LayerNorm()
            (1): Linear(in_features=2048, out_features=16384, bias=False)
            (2): SwiGLU()
            (3): LayerNorm()
            (4): Dropout(p=0.05, inplace=False)
            (5): Linear(in_features=8192, out_features=2048, bias=False)
          )
        )
        (3): ModuleList(
          (0): Attention(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.05, inplace=False)
            (to_q): Linear(in_features=2048, out_features=768, bias=False)
            (to_kv): Linear(in_features=2048, out_features=128, bias=False)
            (rotary_emb): RotaryEmbedding()
            (to_out): Sequential(
              (0): Linear(in_features=768, out_features=2048, bias=False)
              (1): LayerNorm()
            )
          )
          (1): Sequential(
            (0): LayerNorm()
            (1): Linear(in_features=2048, out_features=16384, bias=False)
            (2): SwiGLU()
            (3): LayerNorm()
            (4): Dropout(p=0.05, inplace=False)
            (5): Linear(in_features=8192, out_features=2048, bias=False)
          )
        )
        (4): ModuleList(
          (0): Attention(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.05, inplace=False)
            (to_q): Linear(in_features=2048, out_features=768, bias=False)
            (to_kv): Linear(in_features=2048, out_features=128, bias=False)
            (rotary_emb): RotaryEmbedding()
            (to_out): Sequential(
              (0): Linear(in_features=768, out_features=2048, bias=False)
              (1): LayerNorm()
            )
          )
          (1): Sequential(
            (0): LayerNorm()
            (1): Linear(in_features=2048, out_features=16384, bias=False)
            (2): SwiGLU()
            (3): LayerNorm()
            (4): Dropout(p=0.05, inplace=False)
            (5): Linear(in_features=8192, out_features=2048, bias=False)
          )
        )
        (5): ModuleList(
          (0): Attention(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.05, inplace=False)
            (to_q): Linear(in_features=2048, out_features=768, bias=False)
            (to_kv): Linear(in_features=2048, out_features=128, bias=False)
            (rotary_emb): RotaryEmbedding()
            (to_out): Sequential(
              (0): Linear(in_features=768, out_features=2048, bias=False)
              (1): LayerNorm()
            )
          )
          (1): Sequential(
            (0): LayerNorm()
            (1): Linear(in_features=2048, out_features=16384, bias=False)
            (2): SwiGLU()
            (3): LayerNorm()
            (4): Dropout(p=0.05, inplace=False)
            (5): Linear(in_features=8192, out_features=2048, bias=False)
          )
        )
        (6): ModuleList(
          (0): Attention(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.05, inplace=False)
            (to_q): Linear(in_features=2048, out_features=768, bias=False)
            (to_kv): Linear(in_features=2048, out_features=128, bias=False)
            (rotary_emb): RotaryEmbedding()
            (to_out): Sequential(
              (0): Linear(in_features=768, out_features=2048, bias=False)
              (1): LayerNorm()
            )
          )
          (1): Sequential(
            (0): LayerNorm()
            (1): Linear(in_features=2048, out_features=16384, bias=False)
            (2): SwiGLU()
            (3): LayerNorm()
            (4): Dropout(p=0.05, inplace=False)
            (5): Linear(in_features=8192, out_features=2048, bias=False)
          )
        )
        (7): ModuleList(
          (0): Attention(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.05, inplace=False)
            (to_q): Linear(in_features=2048, out_features=768, bias=False)
            (to_kv): Linear(in_features=2048, out_features=128, bias=False)
            (rotary_emb): RotaryEmbedding()
            (to_out): Sequential(
              (0): Linear(in_features=768, out_features=2048, bias=False)
              (1): LayerNorm()
            )
          )
          (1): Sequential(
            (0): LayerNorm()
            (1): Linear(in_features=2048, out_features=16384, bias=False)
            (2): SwiGLU()
            (3): LayerNorm()
            (4): Dropout(p=0.05, inplace=False)
            (5): Linear(in_features=8192, out_features=2048, bias=False)
          )
        )
        (8): ModuleList(
          (0): Attention(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.05, inplace=False)
            (to_q): Linear(in_features=2048, out_features=768, bias=False)
            (to_kv): Linear(in_features=2048, out_features=128, bias=False)
            (rotary_emb): RotaryEmbedding()
            (to_out): Sequential(
              (0): Linear(in_features=768, out_features=2048, bias=False)
              (1): LayerNorm()
            )
          )
          (1): Sequential(
            (0): LayerNorm()
            (1): Linear(in_features=2048, out_features=16384, bias=False)
            (2): SwiGLU()
            (3): LayerNorm()
            (4): Dropout(p=0.05, inplace=False)
            (5): Linear(in_features=8192, out_features=2048, bias=False)
          )
        )
        (9): ModuleList(
          (0): Attention(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.05, inplace=False)
            (to_q): Linear(in_features=2048, out_features=768, bias=False)
            (to_kv): Linear(in_features=2048, out_features=128, bias=False)
            (rotary_emb): RotaryEmbedding()
            (to_out): Sequential(
              (0): Linear(in_features=768, out_features=2048, bias=False)
              (1): LayerNorm()
            )
          )
          (1): Sequential(
            (0): LayerNorm()
            (1): Linear(in_features=2048, out_features=16384, bias=False)
            (2): SwiGLU()
            (3): LayerNorm()
            (4): Dropout(p=0.05, inplace=False)
            (5): Linear(in_features=8192, out_features=2048, bias=False)
          )
        )
        (10): ModuleList(
          (0): Attention(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.05, inplace=False)
            (to_q): Linear(in_features=2048, out_features=768, bias=False)
            (to_kv): Linear(in_features=2048, out_features=128, bias=False)
            (rotary_emb): RotaryEmbedding()
            (to_out): Sequential(
              (0): Linear(in_features=768, out_features=2048, bias=False)
              (1): LayerNorm()
            )
          )
          (1): Sequential(
            (0): LayerNorm()
            (1): Linear(in_features=2048, out_features=16384, bias=False)
            (2): SwiGLU()
            (3): LayerNorm()
            (4): Dropout(p=0.05, inplace=False)
            (5): Linear(in_features=8192, out_features=2048, bias=False)
          )
        )
        (11): ModuleList(
          (0): Attention(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.05, inplace=False)
            (to_q): Linear(in_features=2048, out_features=768, bias=False)
            (to_kv): Linear(in_features=2048, out_features=128, bias=False)
            (rotary_emb): RotaryEmbedding()
            (to_out): Sequential(
              (0): Linear(in_features=768, out_features=2048, bias=False)
              (1): LayerNorm()
            )
          )
          (1): Sequential(
            (0): LayerNorm()
            (1): Linear(in_features=2048, out_features=16384, bias=False)
            (2): SwiGLU()
            (3): LayerNorm()
            (4): Dropout(p=0.05, inplace=False)
            (5): Linear(in_features=8192, out_features=2048, bias=False)
          )
        )
      )
      (norm): LayerNorm()
      (project_out): Linear(in_features=2048, out_features=34, bias=False)
    )
  )
  (loss_fn): SanityCheck(
    (loss_fn): L1Loss()
  )
)
23-07-10 19:59:09.844 - INFO: Model [DDPM] is created.
23-07-10 19:59:09.844 - INFO: Initial Model Finished
23-07-10 19:59:09.844 - INFO: Starting training from epoch: 0, iter: 0.
23-07-10 19:59:34.197 - INFO: <epoch:  1, iter:      50>
 reg_loss: 2.5578e+02 loss: 2.5578e+02
23-07-10 19:59:57.068 - INFO: <epoch:  1, iter:     100>
 reg_loss: 2.5346e+02 loss: 2.5346e+02
23-07-10 20:00:19.893 - INFO: <epoch:  1, iter:     150>
 reg_loss: 2.4965e+02 loss: 2.4965e+02
23-07-10 20:00:42.449 - INFO: <epoch:  1, iter:     200>
 reg_loss: 2.5637e+02 loss: 2.5637e+02
23-07-10 20:01:04.896 - INFO: <epoch:  1, iter:     250>
 reg_loss: 2.4712e+02 loss: 2.4712e+02
23-07-10 20:01:27.743 - INFO: <epoch:  1, iter:     300>
 reg_loss: 2.2629e+02 loss: 2.2629e+02
23-07-10 20:01:49.987 - INFO: <epoch:  1, iter:     350>
 reg_loss: 2.6999e+02 loss: 2.6999e+02
23-07-10 20:02:12.577 - INFO: <epoch:  1, iter:     400>
 reg_loss: 2.3717e+02 loss: 2.3717e+02
23-07-10 20:02:35.096 - INFO: <epoch:  1, iter:     450>
 reg_loss: 2.5117e+02 loss: 2.5117e+02
23-07-10 20:02:57.302 - INFO: <epoch:  1, iter:     500>
 reg_loss: 2.6379e+02 loss: 2.6379e+02
23-07-10 20:03:19.770 - INFO: <epoch:  1, iter:     550>
 reg_loss: 2.1152e+02 loss: 2.1152e+02
23-07-10 20:03:42.105 - INFO: <epoch:  1, iter:     600>
 reg_loss: 2.5353e+02 loss: 2.5353e+02
23-07-10 20:04:04.252 - INFO: <epoch:  1, iter:     650>
 reg_loss: 2.1023e+02 loss: 2.1023e+02
23-07-10 20:04:26.342 - INFO: <epoch:  1, iter:     700>
 reg_loss: 2.3385e+02 loss: 2.3385e+02
23-07-10 20:04:48.694 - INFO: <epoch:  1, iter:     750>
 reg_loss: 2.1849e+02 loss: 2.1849e+02
23-07-10 20:05:10.857 - INFO: <epoch:  1, iter:     800>
 reg_loss: 2.4440e+02 loss: 2.4440e+02
23-07-10 20:05:33.224 - INFO: <epoch:  1, iter:     850>
 reg_loss: 2.3755e+02 loss: 2.3755e+02
23-07-10 20:05:55.286 - INFO: <epoch:  1, iter:     900>
 reg_loss: 2.1036e+02 loss: 2.1036e+02
23-07-10 20:06:17.709 - INFO: <epoch:  1, iter:     950>
 reg_loss: 2.4636e+02 loss: 2.4636e+02
23-07-10 20:06:39.957 - INFO: <epoch:  1, iter:   1,000>
 reg_loss: 2.0007e+02 loss: 2.0007e+02









































































































































































































sampling loop time step: 100%|██████████| 2000/2000 [06:46<00:00,  4.92it/s]
Traceback (most recent call last):
  File "/home/zhuhe/HPE-with-Diffusion/main.py", line 124, in <module>
    opt['model']['beta_schedule']['train'], schedule_phase='train')
  File "/home/zhuhe/HPE-with-Diffusion/model/model.py", line 173, in validate
    pose_coords = heatmap_to_coord(self.pred_kpt, bbox, idx=i)
  File "/home/zhuhe/HPE-with-Diffusion/core/transforms.py", line 494, in __call__
    return heatmap_to_coord(pred_jts, self.norm_size, bbox, self.output_3d)
  File "/home/zhuhe/HPE-with-Diffusion/core/transforms.py", line 305, in heatmap_to_coord
    assert ndims in [2, 3], "Dimensions of input heatmap should be 2 or 3"
AssertionError: Dimensions of input heatmap should be 2 or 3