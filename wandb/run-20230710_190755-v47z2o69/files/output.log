23-07-10 19:07:57.261 - INFO:   name: sanitycheck
  phase: train
  gpu_ids: [1]
  path:[
    log: experiments/sanitycheck_230710_190754/logs
    checkpoint: experiments/sanitycheck_230710_190754/checkpoint
    resume_state: None
    json_dt: /home/zhuhe/HPE-with-Diffusion/config/dt_val_results.json
    json_gt: /home/zhuhe/HPE-with-Diffusion/config/gt_val_results.json
    experiments_root: experiments/sanitycheck_230710_190754
  ]
  datasets:[
    train:[
      type: mscoco
      root: /home/zhuhe/HPE-with-Diffusion/data/coco/
      img_prefix: images/train2017
      ann: annotations/person_keypoints_train2017.json
      aug:[
        flip: True
        rot_factor: 45
        scale_factor: 0.25
        num_joints_half_body: 3
        prob_half_body: 0.3
      ]
    ]
    val:[
      type: mscoco
      root: /home/zhuhe/HPE-with-Diffusion/data/coco/
      img_prefix: images/val2017
      ann: annotations/person_keypoints_val2017.json
      data_len: 3
    ]
    test:[
      type: mscoco_det
      root: /home/zhuhe/HPE-with-Diffusion/data/coco/
      img_prefix: images/val2017
      det_file: /home/zhuhe/HPE-with-Diffusion/config/test_det_rcnn.json
      ann: annotations/person_keypoints_val2017.json
    ]
  ]
  data_preset:[
    type: simple
    sigma: 2
    num_joints: 17
    image_size: [256, 192]
    heatmap_size: [64, 48]
  ]
  model:[
    regressor:[
      num_layers: 50
    ]
    denoise_transformer:[
      dim: 2048
      num_time_embeds: 1
      num_image_embeds: 4
      num_pose_embeds: 1
      num_keypoints: 17
      casual_transformer:[
        depth: 12
        dim_head: 64
        heads: 12
        ff_mult: 4
        norm_out: True
        attn_dropout: 0.05
        ff_dropout: 0.05
        final_proj: True
        normformer: True
        rotary_emb: True
      ]
    ]
    diffusion:[
      condition_on_preds: True
    ]
    beta_schedule:[
      train:[
        schedule: linear
        n_timestep: 2000
        linear_start: 1e-06
        linear_end: 0.01
      ]
      val:[
        schedule: linear
        n_timestep: 2000
        linear_start: 1e-06
        linear_end: 0.01
      ]
    ]
  ]
  loss:[
    type: sanity_check
    regress: l1
  ]
  train:[
    batch_size: 32
    end_epoch: 270
    optimizer: adam
    lr: 0.001
    lr_factor: 0.1
    lr_step: [170, 200]
    val_freq: 1000
    save_checkpoint_freq: 5000
    print_freq: 100
    ema_scheduler:[
      step_start_ema: 5000
      update_ema_every: 1
      ema_decay: 0.9999
    ]
  ]
  test:[
    heatmap2coord: coord
    batch_size: 32
  ]
  wandb:[
    project_name: DiffHPE
    run_name: sanitycheck
  ]
  distributed: False
loading annotations into memory...
Done (t=3.13s)
creating index...
index created!
23-07-10 19:08:05.296 - INFO: Initial Dataset Finished
loading annotations into memory...
Done (t=0.09s)
creating index...
index created!
/home/zhuhe/.conda/envs/diffhpe/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/zhuhe/.conda/envs/diffhpe/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
23-07-10 19:08:09.291 - INFO: Network G structure: GaussianDiffusion, with parameters: 719,051,762
23-07-10 19:08:09.291 - INFO: GaussianDiffusion(
  (regressor): Regressor(
    (preact): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (avg_pool): AdaptiveAvgPool2d(output_size=1)
    (fc_layer): Linear(
      (linear): Linear(in_features=2048, out_features=34, bias=True)
    )
  )
  (denoise_fn): DenoiseTransformer(
    (to_time_embeds): Sequential(
      (0): PositionalEncoding()
      (1): Linear(in_features=2048, out_features=4096, bias=True)
      (2): Swish()
      (3): Linear(in_features=4096, out_features=2048, bias=True)
      (4): Rearrange('b c (h w) -> b c h w', h=1)
      (5): Rearrange('b c h w -> b (c h) w')
    )
    (to_image_embeds): Sequential(
      (0): Linear(in_features=2048, out_features=8192, bias=True)
      (1): Rearrange('b (n d) -> b n d', n=4)
    )
    (to_pose_embeds): Sequential(
      (0): PositionalEncoding()
      (1): Linear(in_features=2048, out_features=4096, bias=True)
      (2): Swish()
      (3): Linear(in_features=4096, out_features=2048, bias=True)
      (4): Rearrange('b c (h w) -> b c h w', h=1)
      (5): Rearrange('b c h w -> b (c h) w')
    )
    (causal_transformer): CausalTransformer(
      (init_norm): Identity()
      (rel_pos_bias): RelPosBias(
        (relative_attention_bias): Embedding(32, 12)
      )
      (layers): ModuleList(
        (0): ModuleList(
          (0): Attention(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.05, inplace=False)
            (to_q): Linear(in_features=2048, out_features=768, bias=False)
            (to_kv): Linear(in_features=2048, out_features=128, bias=False)
            (rotary_emb): RotaryEmbedding()
            (to_out): Sequential(
              (0): Linear(in_features=768, out_features=2048, bias=False)
              (1): LayerNorm()
            )
          )
          (1): Sequential(
            (0): LayerNorm()
            (1): Linear(in_features=2048, out_features=16384, bias=False)
            (2): SwiGLU()
            (3): LayerNorm()
            (4): Dropout(p=0.05, inplace=False)
            (5): Linear(in_features=8192, out_features=2048, bias=False)
          )
        )
        (1): ModuleList(
          (0): Attention(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.05, inplace=False)
            (to_q): Linear(in_features=2048, out_features=768, bias=False)
            (to_kv): Linear(in_features=2048, out_features=128, bias=False)
            (rotary_emb): RotaryEmbedding()
            (to_out): Sequential(
              (0): Linear(in_features=768, out_features=2048, bias=False)
              (1): LayerNorm()
            )
          )
          (1): Sequential(
            (0): LayerNorm()
            (1): Linear(in_features=2048, out_features=16384, bias=False)
            (2): SwiGLU()
            (3): LayerNorm()
            (4): Dropout(p=0.05, inplace=False)
            (5): Linear(in_features=8192, out_features=2048, bias=False)
          )
        )
        (2): ModuleList(
          (0): Attention(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.05, inplace=False)
            (to_q): Linear(in_features=2048, out_features=768, bias=False)
            (to_kv): Linear(in_features=2048, out_features=128, bias=False)
            (rotary_emb): RotaryEmbedding()
            (to_out): Sequential(
              (0): Linear(in_features=768, out_features=2048, bias=False)
              (1): LayerNorm()
            )
          )
          (1): Sequential(
            (0): LayerNorm()
            (1): Linear(in_features=2048, out_features=16384, bias=False)
            (2): SwiGLU()
            (3): LayerNorm()
            (4): Dropout(p=0.05, inplace=False)
            (5): Linear(in_features=8192, out_features=2048, bias=False)
          )
        )
        (3): ModuleList(
          (0): Attention(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.05, inplace=False)
            (to_q): Linear(in_features=2048, out_features=768, bias=False)
            (to_kv): Linear(in_features=2048, out_features=128, bias=False)
            (rotary_emb): RotaryEmbedding()
            (to_out): Sequential(
              (0): Linear(in_features=768, out_features=2048, bias=False)
              (1): LayerNorm()
            )
          )
          (1): Sequential(
            (0): LayerNorm()
            (1): Linear(in_features=2048, out_features=16384, bias=False)
            (2): SwiGLU()
            (3): LayerNorm()
            (4): Dropout(p=0.05, inplace=False)
            (5): Linear(in_features=8192, out_features=2048, bias=False)
          )
        )
        (4): ModuleList(
          (0): Attention(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.05, inplace=False)
            (to_q): Linear(in_features=2048, out_features=768, bias=False)
            (to_kv): Linear(in_features=2048, out_features=128, bias=False)
            (rotary_emb): RotaryEmbedding()
            (to_out): Sequential(
              (0): Linear(in_features=768, out_features=2048, bias=False)
              (1): LayerNorm()
            )
          )
          (1): Sequential(
            (0): LayerNorm()
            (1): Linear(in_features=2048, out_features=16384, bias=False)
            (2): SwiGLU()
            (3): LayerNorm()
            (4): Dropout(p=0.05, inplace=False)
            (5): Linear(in_features=8192, out_features=2048, bias=False)
          )
        )
        (5): ModuleList(
          (0): Attention(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.05, inplace=False)
            (to_q): Linear(in_features=2048, out_features=768, bias=False)
            (to_kv): Linear(in_features=2048, out_features=128, bias=False)
            (rotary_emb): RotaryEmbedding()
            (to_out): Sequential(
              (0): Linear(in_features=768, out_features=2048, bias=False)
              (1): LayerNorm()
            )
          )
          (1): Sequential(
            (0): LayerNorm()
            (1): Linear(in_features=2048, out_features=16384, bias=False)
            (2): SwiGLU()
            (3): LayerNorm()
            (4): Dropout(p=0.05, inplace=False)
            (5): Linear(in_features=8192, out_features=2048, bias=False)
          )
        )
        (6): ModuleList(
          (0): Attention(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.05, inplace=False)
            (to_q): Linear(in_features=2048, out_features=768, bias=False)
            (to_kv): Linear(in_features=2048, out_features=128, bias=False)
            (rotary_emb): RotaryEmbedding()
            (to_out): Sequential(
              (0): Linear(in_features=768, out_features=2048, bias=False)
              (1): LayerNorm()
            )
          )
          (1): Sequential(
            (0): LayerNorm()
            (1): Linear(in_features=2048, out_features=16384, bias=False)
            (2): SwiGLU()
            (3): LayerNorm()
            (4): Dropout(p=0.05, inplace=False)
            (5): Linear(in_features=8192, out_features=2048, bias=False)
          )
        )
        (7): ModuleList(
          (0): Attention(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.05, inplace=False)
            (to_q): Linear(in_features=2048, out_features=768, bias=False)
            (to_kv): Linear(in_features=2048, out_features=128, bias=False)
            (rotary_emb): RotaryEmbedding()
            (to_out): Sequential(
              (0): Linear(in_features=768, out_features=2048, bias=False)
              (1): LayerNorm()
            )
          )
          (1): Sequential(
            (0): LayerNorm()
            (1): Linear(in_features=2048, out_features=16384, bias=False)
            (2): SwiGLU()
            (3): LayerNorm()
            (4): Dropout(p=0.05, inplace=False)
            (5): Linear(in_features=8192, out_features=2048, bias=False)
          )
        )
        (8): ModuleList(
          (0): Attention(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.05, inplace=False)
            (to_q): Linear(in_features=2048, out_features=768, bias=False)
            (to_kv): Linear(in_features=2048, out_features=128, bias=False)
            (rotary_emb): RotaryEmbedding()
            (to_out): Sequential(
              (0): Linear(in_features=768, out_features=2048, bias=False)
              (1): LayerNorm()
            )
          )
          (1): Sequential(
            (0): LayerNorm()
            (1): Linear(in_features=2048, out_features=16384, bias=False)
            (2): SwiGLU()
            (3): LayerNorm()
            (4): Dropout(p=0.05, inplace=False)
            (5): Linear(in_features=8192, out_features=2048, bias=False)
          )
        )
        (9): ModuleList(
          (0): Attention(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.05, inplace=False)
            (to_q): Linear(in_features=2048, out_features=768, bias=False)
            (to_kv): Linear(in_features=2048, out_features=128, bias=False)
            (rotary_emb): RotaryEmbedding()
            (to_out): Sequential(
              (0): Linear(in_features=768, out_features=2048, bias=False)
              (1): LayerNorm()
            )
          )
          (1): Sequential(
            (0): LayerNorm()
            (1): Linear(in_features=2048, out_features=16384, bias=False)
            (2): SwiGLU()
            (3): LayerNorm()
            (4): Dropout(p=0.05, inplace=False)
            (5): Linear(in_features=8192, out_features=2048, bias=False)
          )
        )
        (10): ModuleList(
          (0): Attention(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.05, inplace=False)
            (to_q): Linear(in_features=2048, out_features=768, bias=False)
            (to_kv): Linear(in_features=2048, out_features=128, bias=False)
            (rotary_emb): RotaryEmbedding()
            (to_out): Sequential(
              (0): Linear(in_features=768, out_features=2048, bias=False)
              (1): LayerNorm()
            )
          )
          (1): Sequential(
            (0): LayerNorm()
            (1): Linear(in_features=2048, out_features=16384, bias=False)
            (2): SwiGLU()
            (3): LayerNorm()
            (4): Dropout(p=0.05, inplace=False)
            (5): Linear(in_features=8192, out_features=2048, bias=False)
          )
        )
        (11): ModuleList(
          (0): Attention(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.05, inplace=False)
            (to_q): Linear(in_features=2048, out_features=768, bias=False)
            (to_kv): Linear(in_features=2048, out_features=128, bias=False)
            (rotary_emb): RotaryEmbedding()
            (to_out): Sequential(
              (0): Linear(in_features=768, out_features=2048, bias=False)
              (1): LayerNorm()
            )
          )
          (1): Sequential(
            (0): LayerNorm()
            (1): Linear(in_features=2048, out_features=16384, bias=False)
            (2): SwiGLU()
            (3): LayerNorm()
            (4): Dropout(p=0.05, inplace=False)
            (5): Linear(in_features=8192, out_features=2048, bias=False)
          )
        )
      )
      (norm): LayerNorm()
      (project_out): Linear(in_features=2048, out_features=34, bias=False)
    )
  )
  (loss_fn): SanityCheck(
    (loss_fn): L1Loss()
  )
)
23-07-10 19:08:09.292 - INFO: Model [DDPM] is created.
23-07-10 19:08:09.292 - INFO: Initial Model Finished
23-07-10 19:08:09.292 - INFO: Starting training from epoch: 0, iter: 0.
23-07-10 19:09:10.906 - INFO: <epoch:  1, iter:     100>
 reg_loss: 2.6968e+02 loss: 2.6968e+02
23-07-10 19:10:11.621 - INFO: <epoch:  1, iter:     200>
 reg_loss: 2.7672e+02 loss: 2.7672e+02
23-07-10 19:11:13.290 - INFO: <epoch:  1, iter:     300>
 reg_loss: 2.1169e+02 loss: 2.1169e+02
23-07-10 19:12:14.526 - INFO: <epoch:  1, iter:     400>
 reg_loss: 2.2308e+02 loss: 2.2308e+02
23-07-10 19:13:16.123 - INFO: <epoch:  1, iter:     500>
 reg_loss: 2.4908e+02 loss: 2.4908e+02
23-07-10 19:14:17.497 - INFO: <epoch:  1, iter:     600>
 reg_loss: 2.4717e+02 loss: 2.4717e+02
23-07-10 19:15:18.637 - INFO: <epoch:  1, iter:     700>
 reg_loss: 2.7711e+02 loss: 2.7711e+02
23-07-10 19:16:22.699 - INFO: <epoch:  1, iter:     800>
 reg_loss: 2.1945e+02 loss: 2.1945e+02
23-07-10 19:17:23.987 - INFO: <epoch:  1, iter:     900>
 reg_loss: 2.0793e+02 loss: 2.0793e+02
23-07-10 19:18:27.032 - INFO: <epoch:  1, iter:   1,000>
 reg_loss: 2.2369e+02 loss: 2.2369e+02
Traceback (most recent call last):
  File "/home/zhuhe/HPE-with-Diffusion/main.py", line 124, in <module>
    diffusion.validate(json_path, heatmap_to_coord, test_loader)
  File "/home/zhuhe/HPE-with-Diffusion/model/model.py", line 167, in validate
    for inps, crop_bboxes, bboxes, img_ids, scores in val_loader:
  File "/home/zhuhe/.conda/envs/diffhpe/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 628, in __next__
    data = self._next_data()
  File "/home/zhuhe/.conda/envs/diffhpe/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 671, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/zhuhe/.conda/envs/diffhpe/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 58, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/zhuhe/.conda/envs/diffhpe/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 58, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/zhuhe/HPE-with-Diffusion/datasets/coco_det.py", line 60, in __getitem__
    image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)  # scipy.misc.imread(img_path, mode='RGB') is depreciated
cv2.error: OpenCV(4.8.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'